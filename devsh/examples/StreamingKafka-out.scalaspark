// Run with 
// $ spark-shell --master local[2]

// Test with 
// $ kafka-topics --create --zookeeper localhost:2181 --partitions 2 --replication-factor 1 --topic status-out
// $ kafka-console-consumer --bootstrap-server localhost:9092 --topic status-out

// Clear out checkpoint directory to rerun
// $ hdfs dfs -rm -r /tmp/kafka-checkpoint


import org.apache.spark.sql.types._

val statusSchema = StructType(List(
  StructField("model", StringType), 
  StructField("dev_id", StringType),
  StructField("temp", IntegerType), 
  StructField("signal", IntegerType), 
  StructField("wifi_status", StringType), 
  StructField("bluetooth_status", StringType), 
  StructField("battery", IntegerType), 
  StructField("latitude", FloatType),
  StructField("longitude", FloatType)))

// simulate stream by reading file set at one file per batch
val statusDF=spark.readStream.schema(statusSchema).option("maxFilesPerTrigger",1).csv("file:///home/training/training_materials/devsh/data/devicestatus_stream/")
statusDF.printSchema

// Create output DF with empty key, with value = columns concatenated to CSV form
val statusValueDF = statusDF.select(lit("").alias("key"),concat_ws(",",$"dev_id",$"model").alias("value"))
statusValueDF.printSchema

// Start query 
// Be sure to clear out checkpoing location before retrying
val statusValueQuery=statusValueDF.writeStream.format("kafka").option("checkpointLocation","/tmp/kafka-checkpoint").option("topic","status-out").option("kafka.bootstrap.servers","localhost:9092").start()
statusValueQuery.stop
