# Upload  to HDFS
# $ hdfs dfs -put $DEVSH/examples/example-data/people.csv
# $ hdfs dfs -put $DEVSH/examples/example-data/people-no-header.csv
# $ hdfs dfs -put $DEVSH/examples/example-data/users.json

# Start Python shell 
# $ pyspark2

# Create a DF from data in memory
myData = (("Josiah","Bartlett"),("Harry","Potter"))
myDF = spark.createDataFrame(myData)
myDF.show()

# Write data to a Hive metastore table called mydata
# $ hdfs dfs -mkdir /devsh_loudacre/mydata
myDF.write.mode("append"). option("path","/devsh_loudacre/mydata").saveAsTable("devsh.my_table")

# Write data as Parquet files in the mydata_parquet directory
myDF.write.save("mydata_parquet")

# Write data as JSON files in the mydata_json directory
myDF.write.json("mydata_json")

# Write data as Avro data format files in the mydata_avro directory
myDF.write.format("avro").save("mydata_avro")

# Display schema
myDF.printSchema()

# Inferring the Schema of a CSV File (No Header)
spark.read.option("inferSchema","true").csv("people-no-header.csv").printSchema()

# Example: Inferring the Schema of a CSV File (with Header)
spark.read.option("inferSchema","true"). option("header","true").csv("people.csv").printSchema()


# Defining a Schema Programmatically 
from pyspark.sql.types import *

columnsList = [
  StructField("pcode", StringType()),
  StructField("lastName", StringType()),
  StructField("firstName", StringType()),
  StructField("age", IntegerType())]

peopleSchema = StructType(columnsList)

peopleDF = spark.read.option("header","true").schema(peopleSchema).csv("people.csv")
peopleDF.printSchema()

# Eager and lazy execution
usersDF = spark.read.json("users.json")
nameAgeDF = usersDF.select("name","age")
nameAgeDF.show()