# Run with 
# $ spark-shell --master local[2]

# Test with 
# $ kafka-topics --create --zookeeper localhost:2181 --partitions 2 --replication-factor 1 --topic status-out
# $  kafka-console-consumer --bootstrap-server localhost:9092 --topic status-out

# $ Clear out checkpoint directory to rerun
# $ hdfs dfs -rm -r /tmp/kafka-checkpoint


from pyspark.sql import functions as f

from pyspark.sql.types import *

statusSchema = StructType([
  StructField("model", StringType()), 
  StructField("dev_id", StringType()),
  StructField("temp", IntegerType()), 
  StructField("signal", IntegerType()), 
  StructField("wifi_status", StringType()), 
  StructField("bluetooth_status", StringType()), 
  StructField("battery", IntegerType()), 
  StructField("latitude", FloatType()),
  StructField("longitude", FloatType())])

# simulate stream by reading file set at one file per batch
statusDF=spark.readStream.schema(statusSchema).option("maxFilesPerTrigger",1).csv("file:///home/training/training_materials/devsh/data/devicestatus_stream/")
statusDF.printSchema()

# Create output DF with empty key, with value = columns concatenated to CSV form
from pyspark.sql import functions
statusValueDF = statusDF.select(functions.lit("").alias("key"),functions.concat_ws(",","dev_id","model").alias("value"))
statusValueDF.printSchema()

# Start query 
# Be sure to clear out checkpoing location before retrying
statusValueQuery=statusValueDF.writeStream.format("kafka").option("checkpointLocation","/tmp/kafka-checkpoint").option("topic","status-out").option("kafka.bootstrap.servers","localhost:9092").start()
statusValueQuery.stop()
