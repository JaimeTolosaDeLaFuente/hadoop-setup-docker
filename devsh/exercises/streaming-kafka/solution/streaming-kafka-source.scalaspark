// Create streaming DataFrame reading from Kafka

// Run with 
// $ spark-shell --master local[2]

// Test with 
// $ kafka-topics --create --zookeeper localhost:2181 --partitions 2 --replication-factor 1 --topic activations
// $ $DEVSH/scripts/streamtest-kafka.sh activations localhost:9092 10 $DEVDATA/activations_stream

val kafkaDF = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "activations").load()
kafkaDF.printSchema()

import org.apache.spark.sql.types._
val activationsSchema = StructType( List(
  StructField("acct_num", IntegerType),
  StructField("dev_id", StringType),
  StructField("phone", StringType),
  StructField("model", StringType))) 

val stringValueDF = kafkaDF.select($"value".cast("string"))
val activationsDF = stringValueDF.select(from_json($"value", activationsSchema).alias("activation"))
activationsDF.printSchema

val activationsQuery = activationsDF.writeStream.outputMode("append").option("truncate","false").format("console").start

activationsQuery.stop

// Count activations by model
val countByModelDF = activationsDF.groupBy("activation.model").count()
// count aggregation only supported with complete output mode
val countByModelQuery = countByModelDF.writeStream.outputMode("complete").format("console").start()

countByModelQuery.stop
